{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c62ede-1ea1-4efb-a656-2aa23bfadb98",
   "metadata": {},
   "source": [
    "See tutorial on https://huggingface.co/blog/ngxson/make-your-own-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ad032-ceb9-473d-bb13-94ed12d4644e",
   "metadata": {},
   "source": [
    "# Loading the dataset - PDF files from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b27f8-3842-4f41-9b24-14cbe2a81c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pdfplumber   ### better result\n",
    "import re\n",
    "\n",
    "\n",
    "# Import Folder containing PDF files (Windows path)\n",
    "DATASET_DIR = Path(r\"C:\\STUDY\\MALD\\RAG\\dataset\")\n",
    "\n",
    "documents = []\n",
    "document_names = []\n",
    "\n",
    "pdf_files = sorted(DATASET_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    raise FileNotFoundError(f\"No .pdf files found in: {DATASET_DIR}\")\n",
    "\n",
    "\n",
    "#########   PDF CLEANING    #########\n",
    "# PDF Cleanup after reading for extraction artefacts (line breaks, column breaks, multiple spaces footnotes, references...)\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    # Cleans raw text extracted best practice for from scientific PDFs.\n",
    "    # The function is conservative: it removes structural noise while preserving semantic content.\n",
    "    \n",
    "    # Fix hyphenation across line breaks eg. \"emo-\\n tional\" -> \"emotional\"\n",
    "    text = re.sub(r\"-\\n\\s*\", \"\", text)\n",
    "\n",
    "    # Remove page headers / footers (heuristic).  Typical patterns: journal names, page numbers\n",
    "    header_footer_patterns = [\n",
    "        r\"^\\s*\\d+\\s*$\",  # page numbers\n",
    "        r\"^.*\\/\\s*(Journal|Proceedings|Conference|Review).*?\\(\\d{4}\\).*$\",\n",
    "        r\"^.*et al\\.\\s*/\\s*.*\\(\\d{4}\\).*$\",\n",
    "    ]\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if any(re.match(p, line.strip()) for p in header_footer_patterns):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "    # Remove footnote markers and symbols\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)        # [1], [2]\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)        # (1)\n",
    "    text = re.sub(r\"\\*+\\s*\", \"\", text)         # *, **\n",
    "\n",
    "\n",
    "    # Stop processing at References / Bibliography\n",
    "    references_pattern = r\"\\n\\s*(References|Bibliography)\\s*\\n\"\n",
    "    split = re.split(references_pattern, text, flags=re.IGNORECASE)\n",
    "    text = split[0]\n",
    "\n",
    "\n",
    "    # Normalize line breaks inside paragraphs\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "\n",
    "    # Collapse excessive whitespace\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "#########   END PDF CLEANING    #########\n",
    "\n",
    "\n",
    "#########   READING PDF #########\n",
    "#### using pdfplumber; faster, cleaner reading for complex text\n",
    "for fp in pdf_files:\n",
    "    pages = []   \n",
    "    with pdfplumber.open(fp) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            raw_text = page.extract_text()\n",
    "            if raw_text:\n",
    "                cleaned = clean_pdf_text(raw_text)\n",
    "                pages.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": cleaned\n",
    "                })\n",
    "    if pages:\n",
    "        documents.append(pages)\n",
    "        document_names.append(fp.name)\n",
    "\n",
    "print(f\"Loaded {len(documents)} PDF documents from {DATASET_DIR}\")\n",
    "print(\"Documents:\")\n",
    "for name in document_names:\n",
    "    print(\" -\", name)\n",
    "#########   END READING PDF #########\n",
    "\n",
    "\n",
    "### SET Minimum chunk length to omit noise, bad chunks,...\n",
    "MIN_CHARS = 100\n",
    "\n",
    "#########   CHUNKING    ########\n",
    "\n",
    "# 1 ##############################################################################################\n",
    "### --- BASIC --- Chunking WITH OVERLEAP: turn each document into multiple smaller chunks for embeddings ---\n",
    "\n",
    "def chunk_text_with_pages(pages, chunk_size: int = 500, overlap: int = 150):\n",
    "    \n",
    "    # Basic character-based chunking with overlap with page-range tracking.\n",
    "    # pages: list of dicts -> {\"page\": int, \"text\": str}\n",
    "    # returns: list of (chunk_text, start_page, end_page)\n",
    "    \n",
    "    # Concatenate pages while keeping page boundaries\n",
    "    full_text = \"\"\n",
    "    page_map = []  # list of (char_start, char_end, page_number)\n",
    "\n",
    "    cursor = 0\n",
    "    for p in pages:\n",
    "        text = p[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        start = cursor\n",
    "        full_text += text + \"\\n\\n\"\n",
    "        cursor = len(full_text)\n",
    "        end = cursor\n",
    "\n",
    "        page_map.append((start, end, p[\"page\"]))\n",
    "\n",
    "    if not full_text.strip():\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    start_char = 0\n",
    "\n",
    "    while start_char < len(full_text):\n",
    "        end_char = min(len(full_text), start_char + chunk_size)\n",
    "        chunk_text = full_text[start_char:end_char].strip()\n",
    "\n",
    "        if chunk_text:\n",
    "            # determine page range for this chunk\n",
    "            pages_covered = [\n",
    "                page\n",
    "                for s, e, page in page_map\n",
    "                if not (e <= start_char or s >= end_char)\n",
    "            ]\n",
    "\n",
    "            if pages_covered:\n",
    "                chunks.append(\n",
    "                    (\n",
    "                        chunk_text,\n",
    "                        min(pages_covered),\n",
    "                        max(pages_covered)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        start_char += step\n",
    "\n",
    "    return chunks\n",
    "\n",
    "### END Chunking with overlap\n",
    "\n",
    "### Preparing dataset for Chunking with overlap \n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "dataset_basic = []\n",
    "\n",
    "for doc_pages, doc_name in zip(documents, document_names):\n",
    "    page_chunks = chunk_text_with_pages(\n",
    "        doc_pages,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    for text, p_start, p_end in page_chunks:\n",
    "        if len(text.strip()) < MIN_CHARS:       # skip short chunks without meaning \n",
    "            continue\n",
    "            \n",
    "        if p_start == p_end:\n",
    "            page_info = f\"PAGE: {p_start}\"\n",
    "        else:\n",
    "            page_info = f\"PAGES: {p_start}-{p_end}\"\n",
    "\n",
    "        dataset_basic.append(\n",
    "            f\"[SOURCE: {doc_name} | {page_info}]\\n{text}\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Prepared {len(dataset_basic)} basic chunks \"\n",
    "    f\"(CHUNK_SIZE={CHUNK_SIZE}, OVERLAP={CHUNK_OVERLAP})\"\n",
    ")\n",
    "### END of Preparing dataset for Chunking with overlap \n",
    "###############################################################################################\n",
    "\n",
    "'''\n",
    "\n",
    "# 2 ###########################################################################################\n",
    "### --- ADVANCED --- Set chunking as paragraph\n",
    "\n",
    "# -------- Paragraph-aware, token-safe chunking --------\n",
    "\n",
    "# Split text into paragraphs based on empty lines.\n",
    "def split_into_paragraphs(text: str):\n",
    "    text = re.sub(r'\\r\\n', '\\n', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 50]   ## \">50\" remove noise: headers, short lines, numbers,...\n",
    "\n",
    "# Split very long paragraphs into fixed-size parts.\n",
    "def hard_split(text: str, max_chars: int):   ## If paragraf size is mode that defined max_chars\n",
    "    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "\n",
    "# Create chunks from a list of pages while preserving page ranges. Each page is a dict with keys: {'page': int, 'text': str}\n",
    "def chunk_by_paragraphs_with_pages(pages, max_chars=1000):\n",
    "    chunks = []\n",
    "    current_text = \"\"\n",
    "    start_page = None\n",
    "    last_page = None\n",
    "\n",
    "    for page in pages:\n",
    "        page_num = page[\"page\"]\n",
    "        paragraphs = split_into_paragraphs(page[\"text\"])\n",
    "\n",
    "        for p in paragraphs:\n",
    "            if len(p) > max_chars:\n",
    "                if current_text:\n",
    "                    chunks.append((current_text.strip(), start_page, last_page))\n",
    "                    current_text = \"\"\n",
    "                    start_page = None\n",
    "                    last_page = None\n",
    "\n",
    "                for part in hard_split(p, max_chars):\n",
    "                    chunks.append((part.strip(), page_num, page_num))\n",
    "                continue\n",
    "\n",
    "            if not current_text:\n",
    "                current_text = p\n",
    "                start_page = page_num\n",
    "                last_page = page_num\n",
    "            elif len(current_text) + len(p) <= max_chars:\n",
    "                current_text += \"\\n\\n\" + p\n",
    "                last_page = page_num\n",
    "            else:\n",
    "                chunks.append((current_text.strip(), start_page, last_page))\n",
    "                current_text = p\n",
    "                start_page = page_num\n",
    "                last_page = page_num\n",
    "\n",
    "    if current_text:\n",
    "        chunks.append((current_text.strip(), start_page, last_page))\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "### END Chunking as paragraph\n",
    "\n",
    "\n",
    "### Preparing dataset for Chunking with overlap \n",
    "# Conservative chunk size, token-safe size for bge-base / similar embedding models\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "# Build dataset from loaded documents for chunking with paragraphs\n",
    "dataset_paragraph = []\n",
    "for doc_pages, doc_name in zip(documents, document_names):\n",
    "    page_chunks = chunk_by_paragraphs_with_pages(doc_pages, max_chars=CHUNK_SIZE)\n",
    "\n",
    "    for text, p_start, p_end in page_chunks:\n",
    "        if len(text.strip()) < MIN_CHARS:       # skip short chunks without meaning \n",
    "            continue\n",
    "        \n",
    "        if p_start == p_end:\n",
    "            page_info = f\"PAGE: {p_start}\"\n",
    "        else:\n",
    "            page_info = f\"PAGES: {p_start}-{p_end}\"\n",
    "\n",
    "        dataset_paragraph.append(\n",
    "            f\"[SOURCE: {doc_name} | {page_info}]\\n{text}\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"Prepared {len(dataset_paragraph)} paragraph-based chunks (CHUNK_SIZE={CHUNK_SIZE})\")\n",
    "\n",
    "### END of Preparing dataset for Chunking with overlap \n",
    "###############################################################################################\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adeeb79-48ce-44dc-a45e-b3c0b5f446ac",
   "metadata": {},
   "source": [
    "# Implement the vector database \n",
    "\n",
    "#### We will use the embedding model from ollama to convert each chunk into an embedding vector, then store the chunk and its corresponding vector in a list.\n",
    "\n",
    "###### EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "###### LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a988d8f-0327-45f9-8aad-24544e9f6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    try:\n",
    "        resp = ollama.embed(model=EMBEDDING_MODEL, input=chunk)\n",
    "        embedding = resp['embeddings'][0] if isinstance(resp, dict) else resp.embeddings[0]\n",
    "        VECTOR_DB.append((chunk, embedding))\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Skipped chunk due to embedding error:\", str(e)[:120])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11abe7-7d11-469d-804b-70b1ffbe2be0",
   "metadata": {},
   "source": [
    "#### Setting chunks in database\n",
    "##### NOTE: \n",
    "##### dataset_basic = chunking with ovelap\n",
    "##### database_paragraph = chunking with paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0438ec7-a491-4859-844b-2a6f13b232c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE variable \"dataset_basic\" OR \"dataset_paragraph\" per used chunking\n",
    "for i, chunk in enumerate(dataset_basic):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset_basic)} to the database')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32764e55-efbc-4926-98bc-9ba5e88436b9",
   "metadata": {},
   "source": [
    "\n",
    "# Implement the retrieval function \n",
    "\n",
    "\n",
    "#### Implementation of retrieval function that takes a query and returns the top N most relevant chunks based on cosine similarity. We can imagine that the higher the cosine similarity between the two vectors, the \"closer\" they are in the vector space. This means they are more similar in terms of meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71203e5d-bc95-40da-bcb9-9b75035a891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Cosine similarity using NumPy library\n",
    "# -----------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(q, d):\n",
    "    return np.dot(q, d) / (np.linalg.norm(q) * np.linalg.norm(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bc32a-beff-459d-b2cf-c1d638907678",
   "metadata": {},
   "source": [
    "#### Implementation of retrieval function:\n",
    "###### Query embedding and retrieval were implemented as separate stages. Queries were embedded once and subsequently matched against the document embedding space using cosine similarity to ensure a controlled and reproducible evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f88f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Embed query\n",
    "# -----------------------------------------------------\n",
    "def embed_query(query: str):\n",
    "    resp = ollama.embed(model=EMBEDDING_MODEL, input=query)\n",
    "    return resp[\"embeddings\"][0] if isinstance(resp, dict) else resp.embeddings[0]\n",
    "\n",
    "\n",
    "## TOP-K \n",
    "# 5-10 for test\n",
    "# 3 for GENERATION\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Retrieve top-k chunks\n",
    "# -----------------------------------------------------\n",
    "def retrieve_top_k(query_embedding, vector_db, k=3):\n",
    "    scored = [\n",
    "        (chunk, cosine_similarity(query_embedding, emb))\n",
    "        for chunk, emb in vector_db\n",
    "    ]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88d3b2-2112-49f9-93a2-cbbe70cd5dd7",
   "metadata": {},
   "source": [
    "# Generation phrase \n",
    "\n",
    "\n",
    "### In this phrase, the chatbot generate a response based on the retrieved knowledge from the step above. This is done by simply add the chunks into the prompt that will be taken as input for the chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46b378-0f0b-4a74-8d93-e48513b8e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "\n",
    "q_emb = embed_query(input_query)\n",
    "top_chunks = retrieve_top_k(q_emb, VECTOR_DB, k=3)\n",
    "print('Retrieved knowledge:')\n",
    "for i, (chunk, score) in enumerate(top_chunks, start=1):\n",
    "    print(f'\\n[{i}] similarity={score:.3f}')\n",
    "    print(f'\\n{chunk}')\n",
    "\n",
    "## To prevent context overflow during generation, retrieved document chunks were concatenated incrementally until a predefined character limit was reached, ensuring stable and reproducible generation behavior.\n",
    "MAX_CONTEXT_CHARS = 3500\n",
    "\n",
    "context_blocks = []\n",
    "current_len = 0\n",
    "\n",
    "for chunk, similarity in top_chunks:                    \n",
    "    if current_len + len(chunk) > MAX_CONTEXT_CHARS:\n",
    "        break\n",
    "    context_blocks.append(chunk)\n",
    "    current_len += len(chunk)\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use ONLY the following context to answer the question.\\n\"\n",
    "    \"If the answer is not contained in the context, say so explicitly.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    + \"\\n\\n\".join(context_blocks)\n",
    "    + f\"\\n\\nQuestion: {input_query}\\nAnswer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb3775-999c-4002-bca3-115ddc00c614",
   "metadata": {},
   "source": [
    "### Generating response with LLM - Ollama. Instruction prompt is used as system message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e2313-8a0e-404e-b5c3-6df2c1e69a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db512ad-2b8d-4d86-ae84-678293ce60bd",
   "metadata": {},
   "source": [
    "### ADDITIONAL:  function for asking a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, top_k=3):\n",
    "    ####################################\n",
    "    # End-to-end RAG query:\n",
    "    # - embeds query\n",
    "    # - retrieves top-k chunks\n",
    "    # - builds safe context\n",
    "    # - generates answer using LLM\n",
    "    ####################################\n",
    "\n",
    "    MAX_CONTEXT_CHARS = 3500\n",
    "\n",
    "    # Embed query\n",
    "    q_emb = embed_query(query)\n",
    "\n",
    "    # Retrieve top-k chunks\n",
    "    retrieved = retrieve_top_k(q_emb, VECTOR_DB, k=top_k)\n",
    "\n",
    "    # Build context incrementally (safe with defined max chars)\n",
    "    context_blocks = []\n",
    "    current_len = 0\n",
    "\n",
    "    for chunk, score in retrieved:\n",
    "        if current_len + len(chunk) > MAX_CONTEXT_CHARS:\n",
    "            break\n",
    "        context_blocks.append(chunk)\n",
    "        current_len += len(chunk)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    # Construct prompt\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant.\\n\"\n",
    "        \"Answer the question using ONLY the information provided in the context.\\n\"\n",
    "        \"If the answer cannot be found in the context, reply:\\n\"\n",
    "        \"\\\"The answer is not contained in the provided documents.\\\"\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Generate answer\n",
    "    response = ollama.generate(\n",
    "        model=\"gemma3:4b\",\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Extract sources\n",
    "    sources = list({\n",
    "        c.split(\"]\")[0] + \"]\"\n",
    "        for c, _ in retrieved\n",
    "        if c.startswith(\"[SOURCE:\")\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"answer\": response[\"response\"],\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f346d9f-443c-4cdf-9e42-015eb481c26b",
   "metadata": {},
   "source": [
    "# Automated PROMPT for prepared questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19bc42b-0e96-4922-90c2-26ec516e0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"Can negative word tone in emails indicate on critical business processes?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e22610",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Evaluation and generating results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94322893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Retrieval Evaluation (annotation-driven, interactive)\n",
    "# =======================\n",
    "# Evaluation questions are taken from annotation keys.\n",
    "# For each question:\n",
    "# 1) the question is printed\n",
    "# 2) retrieval + similarity is computed\n",
    "# 3) the researcher manually annotates relevant results\n",
    "\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Evaluation output logging\n",
    "# -----------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "eval_log_path = f\"retrieval_evaluation_log_{timestamp}.txt\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Cosine similarity using NumPy library\n",
    "# -----------------------------------------------------\n",
    "def cosine_similarity(q, d):\n",
    "    return np.dot(q, d) / (np.linalg.norm(q) * np.linalg.norm(d))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Embed query\n",
    "# -----------------------------------------------------\n",
    "def embed_query(query: str):\n",
    "    resp = ollama.embed(model=EMBEDDING_MODEL, input=query)\n",
    "    return resp[\"embeddings\"][0] if isinstance(resp, dict) else resp.embeddings[0]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Retrieve top-k chunks\n",
    "# -----------------------------------------------------\n",
    "def retrieve_top_k(query_embedding, vector_db, k=10):\n",
    "    scored = [\n",
    "        (chunk, cosine_similarity(query_embedding, emb))\n",
    "        for chunk, emb in vector_db\n",
    "    ]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)       # used descending sorting for similarity order   --> reverse=True\n",
    "    return scored[:k]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Annotation template (questions only, values filled interactively)\n",
    "# -----------------------------------------------------\n",
    "annotations = {\n",
    "    \"Can negative word tone in emails indicate on critical business processes?\": [],\n",
    "    \"What can the long time between receiving an email and replying to it tell us\": [],\n",
    "    \"Is there a connection between critical business processes and long email response times?\": []\n",
    "}\n",
    "\n",
    "k = 10\n",
    "print(\"\\nStarting INTERACTIVE retrieval evaluation (annotation-driven)...\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Formatting output text\n",
    "# -----------------------------------------------------\n",
    "def format_print_chunk(text, width=100, max_chars=1200):    # max output chars limited to 1200 to omit flooding\n",
    "    # Formats chunk text for readable console output:\n",
    "    #   - fixes missing spaces after punctuation\n",
    "    #   - wraps text to fixed line width\n",
    "    #   - limits maximum printed characters\n",
    "    \n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Fix common PDF artifacts (missing spaces)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([.,;:])([A-Za-z])', r'\\1 \\2', text)\n",
    "\n",
    "    # Limit length for display\n",
    "    text = text[:max_chars]\n",
    "\n",
    "    # Wrap nicely\n",
    "    wrapped = textwrap.fill(text, width=width)\n",
    "    return wrapped\n",
    "\n",
    "### formatting chunk for printout\n",
    "def split_source_and_text(chunk: str):\n",
    "    # Splits chunk into SOURCE header and body text. Assumes chunk starts with [SOURCE: ...]\n",
    "    if chunk.startswith(\"[SOURCE:\"):\n",
    "        try:\n",
    "            header, body = chunk.split(\"]\", 1)\n",
    "            return header + \"]\", body.strip()\n",
    "        except ValueError:\n",
    "            return chunk, \"\"\n",
    "    return \"\", chunk\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Interactive evaluation loop\n",
    "# -----------------------------------------------------\n",
    "with open(eval_log_path, \"w\", encoding=\"utf-8\") as f, redirect_stdout(f):       ##### START LOGGING #####\n",
    "\n",
    "    print(\"RETRIEVAL EVALUATION LOG\")\n",
    "    print(f\"Timestamp: {timestamp}\")\n",
    "    print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "    print(f\"ELanguage model: {LANGUAGE_MODEL}\")\n",
    "    print(f\"Used Top-k: {k}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "\n",
    "\n",
    "    for question in annotations.keys():\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"QUESTION:\")\n",
    "        print(question)\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        # 1) Embed query\n",
    "        q_emb = embed_query(question)\n",
    "\n",
    "        # 2) Retrieve top-k\n",
    "        top_chunks = retrieve_top_k(q_emb, VECTOR_DB, k=k)\n",
    "\n",
    "        # 3) Print retrieved chunks with similarity\n",
    "        for i, (chunk, score) in enumerate(top_chunks, start=1):\n",
    "            print(f\"\\n[{i}] similarity={score:.3f}\")\n",
    "            source_header, body_text = split_source_and_text(chunk)\n",
    "\n",
    "            # Source\n",
    "            print(source_header)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            # Text\n",
    "            print(format_print_chunk(body_text))\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "        # 4) Manual relevance judgment\n",
    "        print(\n",
    "            \"Enter positions of relevant results. Example: 1,3 (comma-separated, empty if none are relevant)\"\n",
    "        )\n",
    "\n",
    "        user_input = input(\"Relevant positions: \").strip()\n",
    "\n",
    "        if user_input == \"\":\n",
    "            relevant_positions = []\n",
    "        else:\n",
    "            relevant_positions = [\n",
    "                int(x.strip())\n",
    "                for x in user_input.split(\",\")\n",
    "                if x.strip().isdigit()\n",
    "            ]\n",
    "\n",
    "        annotations[question] = relevant_positions\n",
    "\n",
    "        print(\"Recorded relevant positions:\", relevant_positions)\n",
    "        input(\"Press ENTER to continue to the next question...\")\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Evaluation metrics\n",
    "    # -----------------------------------------------------\n",
    "    def recall_at_k(relevant_positions, k):\n",
    "        return 1 if any(pos <= k for pos in relevant_positions) else 0\n",
    "\n",
    "    def precision_at_k(relevant_positions, k):\n",
    "        return len([pos for pos in relevant_positions if pos <= k]) / k\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Compute aggregated metrics\n",
    "    # -----------------------------------------------------\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "\n",
    "    for q, relevant in annotations.items():\n",
    "        recalls.append(recall_at_k(relevant, k))\n",
    "        precisions.append(precision_at_k(relevant, k))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RETRIEVAL METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Recall@{k}: {np.mean(recalls):.3f}\")\n",
    "    print(f\"Precision@{k}: {np.mean(precisions):.3f}\")\n",
    "    print(f\"Queries evaluated: {len(annotations)}\")\n",
    "\n",
    "\n",
    "print(f\"\\nEvaluation log written to: {eval_log_path}\")          ##### END LOGGING #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
